{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import input_data\n",
    "import tensor_mnist.mnist as input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath:data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "filepath:data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "filepath:data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "filepath:data/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "MNIST ready\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('data/', one_hot=True)\n",
    "trainimg   = mnist.train.images\n",
    "trainlabel = mnist.train.labels\n",
    "testimg    = mnist.test.images\n",
    "testlabel  = mnist.test.labels\n",
    "print (\"MNIST ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* data:n*784  \n",
    "* conv1:卷积核大小3*3*1，卷积核个数64，stride=1*1\n",
    "* pooling1:2*2 ,stride=2*2\n",
    "* conv2:卷积核大小3*3*64(由上一层输入的64个特征图)，卷积核个数128  \n",
    "* pooling2：2*2,stride=2*2  \n",
    "\n",
    "* 全连接层1：1024，全连接层2（分类结果）：10\n",
    "\n",
    "* 输入是28*28大小，计算特征图大小：[h|w-filer(h|w)+2*padding]/stride + 1\n",
    "(28-3+2)/1+1=28,说明卷积层没有改变图像的大小。  \n",
    "\n",
    "* pooling层使图像变小了，2*2变为1*1，h和w都变为原来的1/2，即14*14 \n",
    "* 所以经过第一层pooling后得到的是14*14，  \n",
    "* 第二层pooling后得到的是7*7，特征图是128个。  \n",
    "* 全连接层把特征图转换为向量。输入的是7*7*128输出的是1024维向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input  = 784\n",
    "n_output = 10\n",
    "weights  = {\n",
    "        # 3，3卷积核大小，1输入深度（灰度图深度为1），64：第一层卷积完后得到64个特征图\n",
    "        'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64], stddev=0.1)),\n",
    "        # 输入深度64，输出深度128\n",
    "        'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.1)),\n",
    "        # 7*7*128\n",
    "        'wd1': tf.Variable(tf.random_normal([7*7*128, 1024], stddev=0.1)),\n",
    "        'wd2': tf.Variable(tf.random_normal([1024, n_output], stddev=0.1))\n",
    "    }\n",
    "biases   = {\n",
    "        'bc1': tf.Variable(tf.random_normal([64], stddev=0.1)),\n",
    "        'bc2': tf.Variable(tf.random_normal([128], stddev=0.1)),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024], stddev=0.1)),\n",
    "        'bd2': tf.Variable(tf.random_normal([n_output], stddev=0.1))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stride=(b,h,w,c)参数的含义：  \n",
    "\n",
    "b表示在样本上的步长默认为1，也就是每一个样本都会进行运算，就是不跳过任何一个样本  \n",
    "\n",
    "h表示在高度上的默认移动步长为1，这个可以自己设定，根据网络的结构合理调节。  \n",
    "\n",
    "w表示在宽度上的默认移动步长为1，这个同上可以自己设定。  \n",
    "\n",
    "c表示在通道上的默认移动步长为1，这个表示每一个通道都会进行运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN READY\n"
     ]
    }
   ],
   "source": [
    "def conv_basic(_input, _w, _b, _keepratio):\n",
    "        # INPUT 把输入格式转换为tensorflow格式：tesorflow支持四维格式\n",
    "        # [n,h,w,c] n:batch大小，h图像高，w图像宽，c：深度（图像通道数)\n",
    "        # -1:tensorflow自己推断\n",
    "        _input_r = tf.reshape(_input, shape=[-1, 28, 28, 1])\n",
    "        # 卷积层1\n",
    "        # strides:四维格式，1：batchsize上stride大小，1：在h上大小，1：在w上大小，1：通道上\n",
    "        # padding：same（用0填充），valid(不填充)。区别：same:卷积时不够的0填充，valid：卷积时多余的给扔掉，一般用same\n",
    "        _conv1 = tf.nn.conv2d(_input_r, _w['wc1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        # 可使用tf.nn.moments和tf.nn.batch_normalization进行标准化操作\n",
    "        #_mean, _var = tf.nn.moments(_conv1, [0, 1, 2])\n",
    "        #_conv1 = tf.nn.batch_normalization(_conv1, _mean, _var, 0, 1, 0.0001)\n",
    "        _conv1 = tf.nn.relu(tf.nn.bias_add(_conv1, _b['bc1']))\n",
    "        # 选择2*2窗口，stride也设为2*2\n",
    "        # ksize=[1, 2, 2, 1]，也是四个维度，1：batchsize上大小，2：在h上大小，2：在w上大小，1：通道上\n",
    "        _pool1 = tf.nn.max_pool(_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        _pool_dr1 = tf.nn.dropout(_pool1, _keepratio)\n",
    "        # 卷积层2\n",
    "        _conv2 = tf.nn.conv2d(_pool_dr1, _w['wc2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #_mean, _var = tf.nn.moments(_conv2, [0, 1, 2])\n",
    "        #_conv2 = tf.nn.batch_normalization(_conv2, _mean, _var, 0, 1, 0.0001)\n",
    "        _conv2 = tf.nn.relu(tf.nn.bias_add(_conv2, _b['bc2']))\n",
    "        _pool2 = tf.nn.max_pool(_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        _pool_dr2 = tf.nn.dropout(_pool2, _keepratio)\n",
    "    \n",
    "        # _w['wd1'].get_shape().as_list()[0]拿到7*7*128=6272，改变_pool_dr2的shape，进入全连接层\n",
    "        _dense1 = tf.reshape(_pool_dr2, [-1, _w['wd1'].get_shape().as_list()[0]])\n",
    "        # 全连接层1\n",
    "        _fc1 = tf.nn.relu(tf.add(tf.matmul(_dense1, _w['wd1']), _b['bd1']))\n",
    "        _fc_dr1 = tf.nn.dropout(_fc1, _keepratio)\n",
    "        # 全连接层2，输出结果\n",
    "        _out = tf.add(tf.matmul(_fc_dr1, _w['wd2']), _b['bd2'])\n",
    "        # RETURN\n",
    "        out = { 'input_r': _input_r, 'conv1': _conv1, 'pool1': _pool1, 'pool1_dr1': _pool_dr1,\n",
    "            'conv2': _conv2, 'pool2': _pool2, 'pool_dr2': _pool_dr2, 'dense1': _dense1,\n",
    "            'fc1': _fc1, 'fc_dr1': _fc_dr1, 'out': _out\n",
    "        }\n",
    "        return out\n",
    "print (\"CNN READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6272, 1024]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['wd1'].get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function max_pool in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)\n",
      "    Performs the max pooling on the input.\n",
      "    \n",
      "    Args:\n",
      "      value: A 4-D `Tensor` of the format specified by `data_format`.\n",
      "      ksize: A 1-D int Tensor of 4 elements.  The size of the window for\n",
      "        each dimension of the input tensor.\n",
      "      strides: A 1-D int Tensor of 4 elements.  The stride of the sliding\n",
      "        window for each dimension of the input tensor.\n",
      "      padding: A string, either `'VALID'` or `'SAME'`. The padding algorithm.\n",
      "        See the @{tf.nn.convolution$comment here}\n",
      "      data_format: A string. 'NHWC', 'NCHW' and 'NCHW_VECT_C' are supported.\n",
      "      name: Optional name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of format specified by `data_format`.\n",
      "      The max pooled output tensor.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#print (help(tf.nn.conv2d))\n",
    "print (help(tf.nn.max_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-e5c7e6727672>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "GRAPH READY\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_output])\n",
    "keepratio = tf.placeholder(tf.float32) # 定义dropout率\n",
    "\n",
    "# FUNCTIONS\n",
    "# 定义损失函数和优化项\n",
    "\n",
    "_pred = conv_basic(x, weights, biases, keepratio)['out']\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=_pred, labels=y))\n",
    "optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "_corr = tf.equal(tf.argmax(_pred,1), tf.argmax(y,1)) \n",
    "accr = tf.reduce_mean(tf.cast(_corr, tf.float32)) \n",
    "init = tf.global_variables_initializer()\n",
    "    \n",
    "# SAVER\n",
    "print (\"GRAPH READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/100 cost: 6.843760490\n",
      " Training accuracy: 0.125\n",
      "Epoch: 001/100 cost: 2.816543746\n",
      " Training accuracy: 0.562\n",
      "Epoch: 002/100 cost: 1.483182317\n",
      " Training accuracy: 0.688\n",
      "Epoch: 003/100 cost: 1.221699238\n",
      " Training accuracy: 0.438\n",
      "Epoch: 004/100 cost: 1.114191836\n",
      " Training accuracy: 0.562\n",
      "Epoch: 005/100 cost: 1.130992258\n",
      " Training accuracy: 0.875\n",
      "Epoch: 006/100 cost: 0.970123154\n",
      " Training accuracy: 0.688\n",
      "Epoch: 007/100 cost: 0.874241358\n",
      " Training accuracy: 0.750\n",
      "Epoch: 008/100 cost: 0.846452421\n",
      " Training accuracy: 0.812\n",
      "Epoch: 009/100 cost: 0.577680364\n",
      " Training accuracy: 0.938\n",
      "Epoch: 010/100 cost: 0.679830244\n",
      " Training accuracy: 0.938\n",
      "Epoch: 011/100 cost: 0.731648844\n",
      " Training accuracy: 0.875\n",
      "Epoch: 012/100 cost: 0.642634910\n",
      " Training accuracy: 0.812\n",
      "Epoch: 013/100 cost: 0.570261392\n",
      " Training accuracy: 0.562\n",
      "Epoch: 014/100 cost: 0.630688891\n",
      " Training accuracy: 1.000\n",
      "Epoch: 015/100 cost: 0.483516154\n",
      " Training accuracy: 0.938\n",
      "Epoch: 016/100 cost: 0.374290504\n",
      " Training accuracy: 1.000\n",
      "Epoch: 017/100 cost: 0.487843150\n",
      " Training accuracy: 0.938\n",
      "Epoch: 018/100 cost: 0.407065895\n",
      " Training accuracy: 1.000\n",
      "Epoch: 019/100 cost: 0.337819733\n",
      " Training accuracy: 0.875\n",
      "Epoch: 020/100 cost: 0.502448592\n",
      " Training accuracy: 0.875\n",
      "Epoch: 021/100 cost: 0.474489516\n",
      " Training accuracy: 0.812\n",
      "Epoch: 022/100 cost: 0.316587070\n",
      " Training accuracy: 0.938\n",
      "Epoch: 023/100 cost: 0.524891031\n",
      " Training accuracy: 1.000\n",
      "Epoch: 024/100 cost: 0.482089269\n",
      " Training accuracy: 0.938\n",
      "Epoch: 025/100 cost: 0.351575232\n",
      " Training accuracy: 0.875\n",
      "Epoch: 026/100 cost: 0.296053138\n",
      " Training accuracy: 0.938\n",
      "Epoch: 027/100 cost: 0.424040201\n",
      " Training accuracy: 0.938\n",
      "Epoch: 028/100 cost: 0.320830274\n",
      " Training accuracy: 1.000\n",
      "Epoch: 029/100 cost: 0.265151314\n",
      " Training accuracy: 0.938\n",
      "Epoch: 030/100 cost: 0.175762300\n",
      " Training accuracy: 1.000\n",
      "Epoch: 031/100 cost: 0.260269186\n",
      " Training accuracy: 0.938\n",
      "Epoch: 032/100 cost: 0.330846581\n",
      " Training accuracy: 0.812\n",
      "Epoch: 033/100 cost: 0.137956546\n",
      " Training accuracy: 0.938\n",
      "Epoch: 034/100 cost: 0.151380219\n",
      " Training accuracy: 1.000\n",
      "Epoch: 035/100 cost: 0.198934233\n",
      " Training accuracy: 0.875\n",
      "Epoch: 036/100 cost: 0.190030660\n",
      " Training accuracy: 1.000\n",
      "Epoch: 037/100 cost: 0.215080085\n",
      " Training accuracy: 0.938\n",
      "Epoch: 038/100 cost: 0.185988872\n",
      " Training accuracy: 0.812\n",
      "Epoch: 039/100 cost: 0.144962356\n",
      " Training accuracy: 0.938\n",
      "Epoch: 040/100 cost: 0.171555287\n",
      " Training accuracy: 0.875\n",
      "Epoch: 041/100 cost: 0.310127037\n",
      " Training accuracy: 0.938\n",
      "Epoch: 042/100 cost: 0.208250725\n",
      " Training accuracy: 0.875\n",
      "Epoch: 043/100 cost: 0.176596769\n",
      " Training accuracy: 1.000\n",
      "Epoch: 044/100 cost: 0.155584405\n",
      " Training accuracy: 1.000\n",
      "Epoch: 045/100 cost: 0.232994076\n",
      " Training accuracy: 1.000\n",
      "Epoch: 046/100 cost: 0.216594729\n",
      " Training accuracy: 0.938\n",
      "Epoch: 047/100 cost: 0.271893302\n",
      " Training accuracy: 0.938\n",
      "Epoch: 048/100 cost: 0.191090168\n",
      " Training accuracy: 0.875\n",
      "Epoch: 049/100 cost: 0.165823035\n",
      " Training accuracy: 1.000\n",
      "Epoch: 050/100 cost: 0.312113602\n",
      " Training accuracy: 0.875\n",
      "Epoch: 051/100 cost: 0.172974145\n",
      " Training accuracy: 0.875\n",
      "Epoch: 052/100 cost: 0.163339993\n",
      " Training accuracy: 1.000\n",
      "Epoch: 053/100 cost: 0.076346754\n",
      " Training accuracy: 1.000\n",
      "Epoch: 054/100 cost: 0.164360136\n",
      " Training accuracy: 1.000\n",
      "Epoch: 055/100 cost: 0.129261685\n",
      " Training accuracy: 1.000\n",
      "Epoch: 056/100 cost: 0.197462920\n",
      " Training accuracy: 1.000\n",
      "Epoch: 057/100 cost: 0.127221581\n",
      " Training accuracy: 0.938\n",
      "Epoch: 058/100 cost: 0.231562080\n",
      " Training accuracy: 1.000\n",
      "Epoch: 059/100 cost: 0.184777989\n",
      " Training accuracy: 1.000\n",
      "Epoch: 060/100 cost: 0.234148727\n",
      " Training accuracy: 0.938\n",
      "Epoch: 061/100 cost: 0.186801884\n",
      " Training accuracy: 0.938\n",
      "Epoch: 062/100 cost: 0.109200115\n",
      " Training accuracy: 0.938\n",
      "Epoch: 063/100 cost: 0.127773858\n",
      " Training accuracy: 1.000\n",
      "Epoch: 064/100 cost: 0.119152659\n",
      " Training accuracy: 0.938\n",
      "Epoch: 065/100 cost: 0.171492487\n",
      " Training accuracy: 1.000\n",
      "Epoch: 066/100 cost: 0.086181707\n",
      " Training accuracy: 1.000\n",
      "Epoch: 067/100 cost: 0.235895518\n",
      " Training accuracy: 0.875\n",
      "Epoch: 068/100 cost: 0.167403131\n",
      " Training accuracy: 0.938\n",
      "Epoch: 069/100 cost: 0.158250171\n",
      " Training accuracy: 0.938\n",
      "Epoch: 070/100 cost: 0.088669576\n",
      " Training accuracy: 1.000\n",
      "Epoch: 071/100 cost: 0.106000190\n",
      " Training accuracy: 1.000\n",
      "Epoch: 072/100 cost: 0.122302796\n",
      " Training accuracy: 0.938\n",
      "Epoch: 073/100 cost: 0.142539055\n",
      " Training accuracy: 0.938\n",
      "Epoch: 074/100 cost: 0.130005721\n",
      " Training accuracy: 0.938\n",
      "Epoch: 075/100 cost: 0.115923786\n",
      " Training accuracy: 1.000\n",
      "Epoch: 076/100 cost: 0.125241692\n",
      " Training accuracy: 1.000\n",
      "Epoch: 077/100 cost: 0.120106564\n",
      " Training accuracy: 0.938\n",
      "Epoch: 078/100 cost: 0.130699840\n",
      " Training accuracy: 1.000\n",
      "Epoch: 079/100 cost: 0.173133626\n",
      " Training accuracy: 1.000\n",
      "Epoch: 080/100 cost: 0.159494619\n",
      " Training accuracy: 0.938\n",
      "Epoch: 081/100 cost: 0.139643940\n",
      " Training accuracy: 0.938\n",
      "Epoch: 082/100 cost: 0.067095815\n",
      " Training accuracy: 1.000\n",
      "Epoch: 083/100 cost: 0.102947628\n",
      " Training accuracy: 1.000\n",
      "Epoch: 084/100 cost: 0.077386293\n",
      " Training accuracy: 0.938\n",
      "Epoch: 085/100 cost: 0.092403212\n",
      " Training accuracy: 1.000\n",
      "Epoch: 086/100 cost: 0.069262103\n",
      " Training accuracy: 1.000\n",
      "Epoch: 087/100 cost: 0.067394700\n",
      " Training accuracy: 0.938\n",
      "Epoch: 088/100 cost: 0.145999330\n",
      " Training accuracy: 1.000\n",
      "Epoch: 089/100 cost: 0.140966430\n",
      " Training accuracy: 1.000\n",
      "Epoch: 090/100 cost: 0.089955549\n",
      " Training accuracy: 1.000\n",
      "Epoch: 091/100 cost: 0.054421253\n",
      " Training accuracy: 1.000\n",
      "Epoch: 092/100 cost: 0.093390973\n",
      " Training accuracy: 0.938\n",
      "Epoch: 093/100 cost: 0.101496464\n",
      " Training accuracy: 0.875\n",
      "Epoch: 094/100 cost: 0.109752849\n",
      " Training accuracy: 1.000\n",
      "Epoch: 095/100 cost: 0.125676946\n",
      " Training accuracy: 0.938\n",
      "Epoch: 096/100 cost: 0.033465007\n",
      " Training accuracy: 1.000\n",
      "Epoch: 097/100 cost: 0.113836237\n",
      " Training accuracy: 0.938\n",
      "Epoch: 098/100 cost: 0.096521450\n",
      " Training accuracy: 1.000\n",
      "Epoch: 099/100 cost: 0.152732386\n",
      " Training accuracy: 1.000\n",
      "OPTIMIZATION FINISHED\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "training_epochs = 100\n",
    "batch_size      = 16\n",
    "display_step    = 1\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    #total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # 这里直接取10，为了运行更快一些\n",
    "    total_batch = 10\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys, keepratio:0.7})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keepratio:1.})/total_batch\n",
    "\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0: \n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys, keepratio:1.})\n",
    "        print (\" Training accuracy: %.3f\" % (train_acc))\n",
    "        #test_acc = sess.run(accr, feed_dict={x: testimg, y: testlabel, keepratio:1.})\n",
    "        # print (\" Test accuracy: %.3f\" % (test_acc))\n",
    "\n",
    "print (\"OPTIMIZATION FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
