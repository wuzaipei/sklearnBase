{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "import jieba.posseg as pseg\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# 加载常用停用词\n",
    "stopwords1 = [line.rstrip() for line in open('./中文停用词库.txt', 'r', encoding='utf-8')]\n",
    "# stopwords2 = [line.rstrip() for line in open('./哈工大停用词表.txt', 'r', encoding='utf-8')]\n",
    "# stopwords3 = [line.rstrip() for line in open('./四川大学机器智能实验室停用词库.txt', 'r', encoding='utf-8')]\n",
    "# stopwords = stopwords1 + stopwords2 + stopwords3\n",
    "stopwords = stopwords1\n",
    "\n",
    "\n",
    "def proc_text(raw_line):\n",
    "    \"\"\"\n",
    "        处理每行的文本数据（处理特殊字符，去除停用词）\n",
    "        返回分词结果\n",
    "    \"\"\"\n",
    "    # 1. 使用正则表达式去除非中文字符\n",
    "    filter_pattern = re.compile('[^\\u4E00-\\u9FD5]+')\n",
    "    chinese_only = filter_pattern.sub('', raw_line)\n",
    "\n",
    "    # 2. 结巴分词+词性标注\n",
    "    words_lst = pseg.cut(chinese_only)\n",
    "\n",
    "    # 3. 去除停用词\n",
    "    meaninful_words = []\n",
    "    for word, flag in words_lst:\n",
    "        # if (word not in stopwords) and (flag == 'v'):\n",
    "            # 也可根据词性去除非动词等\n",
    "        if word not in stopwords:\n",
    "            meaninful_words.append(word)\n",
    "\n",
    "    return ' '.join(meaninful_words)\n",
    "\n",
    "\n",
    "def split_train_test(text_df, size=0.8):\n",
    "    \"\"\"\n",
    "        分割训练集和测试集\n",
    "    \"\"\"\n",
    "    # 为保证每个类中的数据能在训练集中和测试集中的比例相同，所以需要依次对每个类进行处理\n",
    "    train_text_df = pd.DataFrame()\n",
    "    test_text_df = pd.DataFrame()\n",
    "\n",
    "    labels = [0, 1, 2, 3]\n",
    "    for label in labels:\n",
    "        # 找出label的记录\n",
    "        text_df_w_label = text_df[text_df['label'] == label]\n",
    "        # 重新设置索引，保证每个类的记录是从0开始索引，方便之后的拆分\n",
    "        text_df_w_label = text_df_w_label.reset_index()\n",
    "\n",
    "        # 默认按80%训练集，20%测试集分割\n",
    "        # 这里为了简化操作，取前80%放到训练集中，后20%放到测试集中\n",
    "        # 当然也可以随机拆分80%，20%（尝试实现下DataFrame中的随机拆分）\n",
    "\n",
    "        # 该类数据的行数\n",
    "        n_lines = text_df_w_label.shape[0]\n",
    "        split_line_no = math.floor(n_lines * size)\n",
    "        text_df_w_label_train = text_df_w_label.iloc[:split_line_no, :]\n",
    "        text_df_w_label_test = text_df_w_label.iloc[split_line_no:, :]\n",
    "\n",
    "        # 放入整体训练集，测试集中\n",
    "        train_text_df = train_text_df.append(text_df_w_label_train)\n",
    "        test_text_df = test_text_df.append(text_df_w_label_test)\n",
    "\n",
    "    train_text_df = train_text_df.reset_index()\n",
    "    test_text_df = test_text_df.reset_index()\n",
    "    return train_text_df, test_text_df\n",
    "\n",
    "\n",
    "def get_word_list_from_data(text_df):\n",
    "    \"\"\"\n",
    "        将数据集中的单词放入到一个列表中\n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    for _, r_data in text_df.iterrows():\n",
    "        word_list += r_data['text'].split(' ')\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def extract_feat_from_data(text_df, text_collection, common_words_freqs):\n",
    "    \"\"\"\n",
    "        特征提取\n",
    "    \"\"\"\n",
    "    # 这里只选择TF-IDF特征作为例子\n",
    "    # 可考虑使用词频或其他文本特征作为额外的特征\n",
    "\n",
    "    n_sample = text_df.shape[0]\n",
    "    n_feat = len(common_words_freqs)\n",
    "    common_words = [word for word, _ in common_words_freqs]\n",
    "\n",
    "    # 初始化\n",
    "    X = np.zeros([n_sample, n_feat])\n",
    "    y = np.zeros(n_sample)\n",
    "\n",
    "    print('提取特征...')\n",
    "    for i, r_data in text_df.iterrows():\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print('已完成{}个样本的特征提取'.format(i + 1))\n",
    "\n",
    "        text = r_data['text']\n",
    "\n",
    "        feat_vec = []\n",
    "        for word in common_words:\n",
    "            if word in text:\n",
    "                # 如果在高频词中，计算TF-IDF值\n",
    "                tf_idf_val = text_collection.tf_idf(word, text)\n",
    "            else:\n",
    "                tf_idf_val = 0\n",
    "\n",
    "            feat_vec.append(tf_idf_val)\n",
    "\n",
    "        # 赋值\n",
    "        X[i, :] = np.array(feat_vec)\n",
    "        y[i] = int(r_data['label'])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def cal_acc(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "        计算准确率\n",
    "    \"\"\"\n",
    "    n_total = len(true_labels)\n",
    "    correct_list = [true_labels[i] == pred_labels[i] for i in range(n_total)]\n",
    "\n",
    "    acc = sum(correct_list) / n_total\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tools import proc_text, split_train_test, get_word_list_from_data, \\\n",
    "    extract_feat_from_data, cal_acc\n",
    "from nltk.text import TextCollection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "dataset_path = './dataset'\n",
    "text_filenames = ['0_simplifyweibo.txt', '1_simplifyweibo.txt',\n",
    "                  '2_simplifyweibo.txt', '3_simplifyweibo.txt']\n",
    "\n",
    "# 原始数据的csv文件\n",
    "output_text_filename = 'raw_weibo_text.csv'\n",
    "\n",
    "# 清洗好的文本数据文件\n",
    "output_cln_text_filename = 'clean_weibo_text.csv'\n",
    "\n",
    "# 处理和清洗文本数据的时间较长，通过设置is_first_run进行配置\n",
    "# 如果是第一次运行需要对原始文本数据进行处理和清洗，需要设为True\n",
    "# 如果之前已经处理了文本数据，并已经保存了清洗好的文本数据，设为False即可\n",
    "is_first_run = True\n",
    "\n",
    "\n",
    "def read_and_save_to_csv():\n",
    "    \"\"\"\n",
    "        读取原始文本数据，将标签和文本数据保存成csv\n",
    "    \"\"\"\n",
    "\n",
    "    text_w_label_df_lst = []\n",
    "    for text_filename in text_filenames:\n",
    "        text_file = os.path.join(dataset_path, text_filename)\n",
    "\n",
    "        # 获取标签，即0, 1, 2, 3\n",
    "        label = int(text_filename[0])\n",
    "\n",
    "        # 读取文本文件\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        labels = [label] * len(lines)\n",
    "        # 把文本和标签都转为series\n",
    "        text_series = pd.Series(lines)\n",
    "        label_series = pd.Series(labels)\n",
    "\n",
    "        # 把数据转为dataframe\n",
    "        text_w_label_df = pd.concat([label_series, text_series], axis=1)\n",
    "        text_w_label_df_lst.append(text_w_label_df)\n",
    "\n",
    "    result_df = pd.concat(text_w_label_df_lst, axis=0)\n",
    "\n",
    "    # 把数据保存成csv文件\n",
    "    result_df.columns = ['label', 'text']\n",
    "    result_df.to_csv(os.path.join(dataset_path, output_text_filename),\n",
    "                     index=None, encoding='utf-8')\n",
    "\n",
    "\n",
    "def run_main():\n",
    "    is_first_run = False\n",
    "    \"\"\"\n",
    "        主函数\n",
    "    \"\"\"\n",
    "    # 1. 数据读取，处理，清洗，准备   （数据处理需要很长时间）\n",
    "    if is_first_run:\n",
    "        print('处理清洗文本数据中...', end=' ')\n",
    "        # 如果是第一次运行需要对原始文本数据进行处理和清洗\n",
    "\n",
    "        # 读取原始文本数据，将标签和文本数据保存成csv\n",
    "        read_and_save_to_csv()\n",
    "\n",
    "        # 读取处理好的csv文件，构造数据集\n",
    "        text_df = pd.read_csv(os.path.join(dataset_path, output_text_filename),\n",
    "                              encoding='utf-8')\n",
    "\n",
    "        # 处理文本数据（去除特殊字符，去除停用词，作分词）\n",
    "        text_df['text'] = text_df['text'].apply(proc_text)\n",
    "\n",
    "        # 过滤空字符串\n",
    "        text_df = text_df[text_df['text'] != '']\n",
    "\n",
    "        # 保存处理好的文本数据\n",
    "        text_df.to_csv(os.path.join(dataset_path, output_cln_text_filename),\n",
    "                       index=None, encoding='utf-8')\n",
    "        print('完成，并保存结果。')\n",
    "\n",
    "    # 2. 分割训练集、测试集\n",
    "    print('加载处理好的文本数据')\n",
    "    clean_text_df = pd.read_csv(os.path.join(dataset_path, output_cln_text_filename),\n",
    "                                encoding='utf-8')\n",
    "    # 分割训练集和测试集\n",
    "    train_text_df, test_text_df = split_train_test(clean_text_df)\n",
    "    # 查看训练集测试集基本信息\n",
    "    print('训练集中各类的数据个数：', train_text_df.groupby('label').size())\n",
    "    print('测试集中各类的数据个数：', test_text_df.groupby('label').size())\n",
    "\n",
    "    # 3. 特征提取\n",
    "    # 计算词频\n",
    "    n_common_words = 200\n",
    "\n",
    "    # 将训练集中的单词拿出来统计词频\n",
    "    print('统计词频...')\n",
    "    all_words_in_train = get_word_list_from_data(train_text_df)\n",
    "    fdisk = nltk.FreqDist(all_words_in_train)\n",
    "    common_words_freqs = fdisk.most_common(n_common_words)\n",
    "    print('出现最多的{}个词是：'.format(n_common_words))\n",
    "    for word, count in common_words_freqs:\n",
    "        print('{}: {}次'.format(word, count))\n",
    "    print()\n",
    "\n",
    "    # 在训练集上提取特征\n",
    "    # text_collection对象有tf-idf方法\n",
    "    text_collection = TextCollection(train_text_df['text'].values.tolist())\n",
    "    print('训练样本提取特征...', end=' ')\n",
    "    train_X, train_y = extract_feat_from_data(train_text_df, text_collection, common_words_freqs)\n",
    "    print('完成')\n",
    "    print()\n",
    "\n",
    "    print('测试样本提取特征...', end=' ')\n",
    "    test_X, test_y = extract_feat_from_data(test_text_df, text_collection, common_words_freqs)\n",
    "    print('完成')\n",
    "\n",
    "    # 4. 训练模型Naive Bayes\n",
    "    print('训练模型...', end=' ')\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train_X, train_y)\n",
    "    print('完成')\n",
    "    print()\n",
    "    # 5. 预测\n",
    "    print('测试模型...', end=' ')\n",
    "    test_pred = gnb.predict(test_X)\n",
    "    print('完成')\n",
    "    # 输出准确率\n",
    "    print('准确率：', cal_acc(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准确率很低，改进空间很大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
